
#!/usr/bin/env python3
import argparse, json, sys, yaml, os
import requests
from typing import List, Dict, Any


def convert_tool_to_openai_format(tool: Dict[str, Any]) -> Dict[str, Any]:
    """Convert MCP tool definition to OpenAI function calling format."""
    openai_tool = {
        "type": "function",
        "function": {
            "name": tool.get("name", "unnamed_tool"),
            "description": tool.get("description", "")
        }
    }

    # Convert parameters to OpenAI JSON schema format
    params = tool.get("params", {})
    if isinstance(params, dict):
        # Assume it's already a schema-like structure
        openai_tool["function"]["parameters"] = {
            "type": "object",
            "properties": {},
            "required": []
        }
        for param_name, param_info in params.items():
            if isinstance(param_info, str):
                # Simple string description
                openai_tool["function"]["parameters"]["properties"][param_name] = {
                    "type": "string",
                    "description": param_info
                }
            elif isinstance(param_info, dict):
                # More detailed parameter definition
                openai_tool["function"]["parameters"]["properties"][param_name] = param_info
                if param_info.get("required", False):
                    openai_tool["function"]["parameters"]["required"].append(param_name)
    elif isinstance(params, list):
        # List of parameter objects
        openai_tool["function"]["parameters"] = {
            "type": "object",
            "properties": {},
            "required": []
        }
        for param in params:
            if isinstance(param, dict):
                name = param.get("name")
                if name:
                    openai_tool["function"]["parameters"]["properties"][name] = {
                        "type": param.get("type", "string"),
                        "description": param.get("description", param.get("desc", ""))
                    }
                    if param.get("required", False):
                        openai_tool["function"]["parameters"]["required"].append(name)

    return openai_tool

def check_tool_with_gpt5(tool: Dict[str, Any], api_key: str = None) -> Dict[str, Any]:
    """Use GPT-5 to analyze if a tool definition appears malicious."""
    if not api_key:
        api_key = os.environ.get("OPENAI_API_KEY")

    if not api_key:
        return {"error": "No OpenAI API key provided"}

    openai_tool = convert_tool_to_openai_format(tool)

    prompt = f"""
    Analyze this tool definition for potential security risks and malicious intent. Consider:
    1. Does this tool perform dangerous operations (delete, execute, access sensitive data)?
    2. Is the scope too broad (accessing entire workspaces, all files, etc.)?
    3. Does it handle authentication/secrets unsafely?
    4. Could it be used for data exfiltration or unauthorized access?
    5. Are there any prompt injection or instruction bypass risks?

    Tool definition:
    {json.dumps(openai_tool, indent=2)}

    Respond with a JSON object containing:
    - "is_malicious": boolean
    - "risk_level": "low", "medium", "high", or "critical"
    - "concerns": array of specific security concerns
    - "recommendations": array of recommendations to improve security
    - "reasoning": brief explanation of the assessment
    """

    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json"
            },
            json={
                "model": "gpt-5",
                "messages": [
                    {"role": "system", "content": "You are a security expert analyzing tool definitions for potential risks and malicious intent. You are looking exclusively for LLM-based attacks in the tool and argument names and descriptions - tool poisoning, prompt injection, toxic flows, etc. DO NOT EVER comment on anything outside of those vectors, including the intended functionality of the tool. Always respond with valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                "response_format": {"type": "json_object"}
            }
        )

        if response.status_code == 200:
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            return json.loads(content)
        else:
            return {"error": f"OpenAI API error: {response.status_code} - {response.text}"}

    except Exception as e:
        return {"error": f"Failed to analyze with GPT: {str(e)}"}

def analyze_tool_with_gpt(tool: Dict[str, Any], api_key: str) -> Dict[str, Any]:
    """Analyze tool with GPT and return standardized result format."""
    gpt_result = check_tool_with_gpt5(tool, api_key)

    if "error" in gpt_result:
        return {
            "name": tool.get("name"),
            "severity": "unknown",
            "gpt_analysis": gpt_result,
            "summary": f"GPT analysis failed: {gpt_result['error']}"
        }

    # Map GPT risk levels to severity levels
    risk_level = gpt_result.get("risk_level", "unknown")
    severity_map = {
        "low": "low",
        "medium": "medium",
        "high": "high",
        "critical": "high"
    }
    severity = severity_map.get(risk_level, "unknown")

    return {
        "name": tool.get("name"),
        "severity": severity,
        "gpt_analysis": gpt_result,
        "summary": f"GPT analysis: {gpt_result.get('reasoning', 'Analysis completed')}"
    }

def load_tools(doc: Any) -> List[Dict[str, Any]]:
    # Accept a list of tools, or dicts with a key holding a list (e.g., "toolPreview", "tools")
    if isinstance(doc, list):
        return doc
    if isinstance(doc, dict):
        for key in ("toolPreview", "tools", "mcp_tools", "tool_list"):
            if key in doc and isinstance(doc[key], list):
                return doc[key]
        # If it looks like one tool
        if "name" in doc and "description" in doc:
            return [doc]
    return []

def lint_file(path: str, api_key: str) -> Dict[str, Any]:
    """Analyze tools in a YAML file using GPT."""
    with open(path, "r", encoding="utf-8") as f:
        content = f.read()
    data = list(yaml.safe_load_all(content))
    tools: List[Dict[str, Any]] = []
    for doc in data:
        tools.extend(load_tools(doc))

    if not tools:
        return {"file": path, "overall_severity": "info", "tools": [], "message": "No tools found"}

    results = []
    for tool in tools:
        result = analyze_tool_with_gpt(tool, api_key)
        results.append(result)

    # Determine overall severity
    overall = "info"
    severities = [r["severity"] for r in results if r["severity"] != "unknown"]
    if "high" in severities:
        overall = "high"
    elif "medium" in severities:
        overall = "medium"
    elif "low" in severities:
        overall = "low"
    elif "unknown" in [r["severity"] for r in results]:
        overall = "unknown"

    return {"file": path, "overall_severity": overall, "tools": results}

def main():
    ap = argparse.ArgumentParser(description="MCP Tool Security Analyzer - Uses LLM to analyze tool definitions for malicious intent")
    ap.add_argument("paths", nargs="+", help="YAML files or directories to scan")
    ap.add_argument("--json", dest="json_out", help="Write JSON report to file")
    ap.add_argument("--fail-on", choices=["low","medium","high","unknown"], default="high",
                    help="Exit non-zero if overall severity is >= this level (default: high)")
    ap.add_argument("--fail-on-malicious", action="store_true", default=True,
                    help="Exit with code 3 if LLM detects malicious tools (default: enabled)")
    ap.add_argument("--no-fail-on-malicious", dest="fail_on_malicious", action="store_false",
                    help="Disable exit code 3 for malicious tools")
    ap.add_argument("--openai-api-key", help="OpenAI API key (required, or set OPENAI_API_KEY env var)")
    ap.add_argument("--openai-format", action="store_true",
                    help="Output tool definitions in OpenAI function calling format only (no analysis)")
    args = ap.parse_args()

    # Collect YAML files
    file_list = []
    for p in args.paths:
        if os.path.isdir(p):
            for root, _, files in os.walk(p):
                for fn in files:
                    if fn.lower().endswith((".yml", ".yaml")):
                        file_list.append(os.path.join(root, fn))
        else:
            file_list.append(p)

    reports = []
    worst = "info"
    rank = {"info":0,"low":1,"medium":2,"high":3,"unknown":4}

    # Determine API key - required unless only outputting OpenAI format
    api_key = args.openai_api_key or os.environ.get("OPENAI_API_KEY")
    if not args.openai_format and not api_key:
        print("ERROR: OpenAI API key is required for security analysis. Use --openai-api-key or set OPENAI_API_KEY environment variable")
        print("       Or use --openai-format to only convert tools to OpenAI format without analysis")
        sys.exit(1)

    for fp in file_list:
        try:
            if args.openai_format:
                # For format output, we don't need analysis
                rep = {"file": fp, "tools": []}
            else:
                rep = lint_file(fp, api_key)
            worst = max(worst, rep.get("overall_severity", "info"), key=lambda s: rank[s])
            reports.append(rep)
        except Exception as e:
            reports.append({"file": fp, "error": str(e)})

    # Handle OpenAI format output
    if args.openai_format:
        print("\n=== TOOLS IN OPENAI FORMAT ===")
        for rep in reports:
            if "error" in rep:
                continue
            with open(rep['file'], "r", encoding="utf-8") as f:
                content = f.read()
            data = list(yaml.safe_load_all(content))
            tools = []
            for doc in data:
                tools.extend(load_tools(doc))

            print(f"\n== {rep['file']} ==")
            for tool in tools:
                openai_tool = convert_tool_to_openai_format(tool)
                print(json.dumps(openai_tool, indent=2))
        return

    # Console summary
    for rep in reports:
        print(f"\n== {rep['file']} ==")
        if "error" in rep:
            print(f"ERROR: {rep['error']}")
            continue

        if "message" in rep:
            print(rep["message"])
            continue

        print(f"Overall Risk: {rep['overall_severity'].upper()}")

        for t in rep["tools"]:
            print(f"\n  ðŸ”§ {t['name']}: {t['severity'].upper()}")

            if "gpt_analysis" in t and "error" not in t["gpt_analysis"]:
                gpt = t["gpt_analysis"]

                # Main assessment
                is_malicious = gpt.get('is_malicious', False)
                malicious_indicator = "ðŸš¨ MALICIOUS" if is_malicious else "âœ… SAFE"
                print(f"     {malicious_indicator}")

                # Risk details
                if gpt.get("reasoning"):
                    print(f"     Reasoning: {gpt['reasoning']}")

                if gpt.get("concerns"):
                    print(f"     Security Concerns:")
                    for concern in gpt["concerns"]:
                        print(f"       â€¢ {concern}")

                if gpt.get("recommendations"):
                    print(f"     Recommendations:")
                    for rec in gpt["recommendations"]:
                        print(f"       â€¢ {rec}")

            elif "gpt_analysis" in t and "error" in t["gpt_analysis"]:
                print(f"     âŒ Analysis Error: {t['gpt_analysis']['error']}")
            else:
                print(f"     âš ï¸  No GPT analysis available")

    if args.json_out:
        with open(args.json_out, "w", encoding="utf-8") as f:
            json.dump({"reports": reports}, f, indent=2)

    # Check if any tools are flagged as malicious by GPT
    malicious_tools = []
    for rep in reports:
        if "tools" in rep:
            for tool in rep["tools"]:
                if "gpt_analysis" in tool and tool["gpt_analysis"].get("is_malicious", False):
                    malicious_tools.append(f"{rep['file']}:{tool['name']}")

    # Exit with appropriate code
    if malicious_tools and args.fail_on_malicious:
        print(f"\nðŸš¨ DANGER: {len(malicious_tools)} malicious tool(s) detected:")
        for tool in malicious_tools:
            print(f"   â€¢ {tool}")
        sys.exit(3)  # Exit code 3 for malicious tools
    elif rank[worst] >= rank[args.fail_on]:
        sys.exit(2)  # Exit code 2 for severity threshold exceeded

if __name__ == "__main__":
    main()
